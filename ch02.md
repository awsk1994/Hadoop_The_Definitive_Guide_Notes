# 2 关于MapReduce

 - MapReduce是一种可用于数据处理的编程模型（programming model)
 - 该模型比较简单，但要想写出有用的程序却不容易
 - Hadoop可以运行各种语言版本的MapReduce
 - MapReduce本质上是并行运行（Parallel Processing)，因此可以将大规模的数据分析任务分给任何一个拥有足够多机器的数据中心。

## 2.1 气象数据集

 - 这里我们展示一个例子：一个分析大量气象数据的程序

## 2.2 使用Unix工具来分析数据

 - 每年全球气温的最高记录是多少？我们先不使用Hadoop解决这个问题
 - 以下的程序可以从记录中找出每年最高气温
```bash
#!/usr/bin/env bash
for year in all/* do
echo -ne `basename $year .gz`"\t" gunzip -c $year | \
    awk '{ temp = substr($0, 88, 5) + 0;
           q = substr($0, 93, 1);
if (temp !=9999 && q ~ /[01459]/ && temp > max) max = temp } END { print max }'
done
```
 - 这个脚本循环遍历按年压缩的所有数据文件
   - 从每个数据文件中取出年份和气温，检查数据是否正确，然后拿该值将目前读取的最高气温进行比较。如果该值比原先的大，就替换目前的最高气温值。
 - 使用亚马逊的EC2 High-CPU Extra Large Instance运行这个程序，需要42分钟。
 
**有进步空间？**

1. 使用计算机的所有可用的硬件线程（hardware threads）并行处理；每个线程负责不同的年份
 - 但这个方法有问题：
   - 由于不同的年份数据文件大小有差异，所以将任务划分成大小相同的作业通常不是一件容易的事情
   - 假设不划分成大小相同，一部分的线程会比其他线程更早结束运行。最后总运行时间仍然取决于处理最长文件所需要的时间

2. 将输入数据分成固定大小的块（chunks），然后每块分到各个进程进行
 - 此外，还需要一个合并各个独立进程的结果的步骤


3. 使用多台计算机
 - 但这个做法引来了协调性和可靠性的问题。
   - 哪个进程负责运行整个作业？
   - 我们如何处理失败的进程？
 - 因此，虽然这是可行，实际上却很麻烦；我们可以借助于Hadoop类似的架构解决这个问题

## 2.3 使用Hadoop来分析数据

 - 首先，我们需要将查询表示成MapReduce作业。

### 2.3.1 Map and Reduce

 - MapReduce任务过程分为两个处理阶段：map和reduce
   - 每个阶段都以键-值对作为输入和输出
   - 程序员还需要写两个函数：map函数和reduce函数
  
 - Map阶段的输入是原始数据
   - Map函数很简单：遍历每行数据，如数据正确，取出年份和气温（如数据不正确或缺失，忽略它）
   - Map函数只是一个数据准备阶段，使Reduce函数能够对他进行处理

 - Map函数的输出经由MapReduce框架处理，当中基于键来对键-值对进行排序和分组
```
# Before
(1950, 0)
(1950, 22)
(1950, -11)
(1951, 4)
(1951, 12)
(1952, 2)

# After Sorting + Grouping

(1950, [0, 22, -11])
(1951, [4, 12])
(1952, [2])
```

 - Reduce函数只需要遍历整个列表并从中找最大的气温
```
# Reduce Output
(1950, 22)
(1951, 12)
(1952, 2)
```

<img src="./img/MapReduce逻辑数据流.png" height="300px"/>

## 2.3.2 Java MapReduce

 - Map函数是由Mapper类表示的：

```java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.LongWritable; 
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MaxTemperatureMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
  private static final int MISSING = 9999;

  @Override
  public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
    String line = value.toString();
    String year = line.substring(15, 19);
    int airTemperature;

    if (line.charAt(87) == '+') { // parseInt doesn't like leading plus signs
      airTemperature = Integer.parseInt(line.substring(88, 92)); 
    } else {
      airTemperature = Integer.parseInt(line.substring(87, 92)); 
    }
    
    String quality = line.substring(92, 93);
    if (airTemperature != MISSING && quality.matches("[01459]")) {
      context.write(new Text(year), new IntWritable(airTemperature)); 
    }
  }
}
```
 - 解释：
   - Hadoop本身提供了一套可优化网络序列化传输的基本类型（在org.apache.hadoop.io package)
   - Map函数的输入是键值和Context
   - Context实例用于输出内容的写入

 - Reduce函数用Reducer类来定义：

```java

import java.io.IOException;
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class MaxTemperatureReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
  @Override
  public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
    int maxValue = Integer.MIN_VALUE; 

    for (IntWritable value : values) {
      maxValue = Math.max(maxValue, value.get()); 
    }

    context.write(key, new IntWritable(maxValue)); 
  }
}
```

 - 第三部分代码负责运行MapReduce作业：

```java
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class MaxTemperature {
  public static void main(String[] args) throws Exception {
    if (args.length != 2) {
      System.err.println("Usage: MaxTemperature <input path> <output path>");
      System.exit(-1); 
    }
    
    Job job = new Job(); 

    job.setJarByClass(MaxTemperature.class); 
    job.setJobName("Max temperature");

    FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.setMapperClass(MaxTemperatureMapper.class); 
    job.setReducerClass(MaxTemperatureReducer.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    System.exit(job.waitForCompletion(true) ? 0 : 1); 
  }
}
```


